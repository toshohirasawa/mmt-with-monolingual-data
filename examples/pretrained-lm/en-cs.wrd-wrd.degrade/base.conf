[train]
model_type:      MultimodalTransformer
max_epochs:      500
batch_size:      2000
eval_batch_size: 1500
update_freq:     2

optimizer:       transformer
# set lr==0 to use model_dim^-0.5
lr:              0

eval_metrics:    bleu,meteor,loss
eval_beam:       5
eval_freq:       0
eval_max_len:    100
eval_filters:
patience:        10

[model]
# dummy opts; misc.py requires *_dim when using *_type
dec_dim:   512
enc_dim:   512

direction: en,lx:Lxmert->cs
feat_names: lx.l,lx.r
feat_dim:  768
# set feat_ratio==0 to consider only text modality
feat_ratio: 0

bucket_by: en
sampler_type: token

[data]
tok:   ../data/tok
lxm:   ../data/lxmert
brt:   ../data/bert
obj:   ../data/faster_rcnn
msk:   ../data/mask
train_set: {
    'en': '${msk}/train.lc.norm.tok.entity.en',
    'cs': '${tok}/train.lc.norm.tok.cs',
    'lx': '${lxm}/mask/train.tok22.obj36.pt'}

val_set: {
    'en': '${msk}/val.lc.norm.tok.entity.en',
    'cs': '${tok}/val.lc.norm.tok.cs',
    'lx': '${lxm}/mask/val.tok22.obj36.pt'}

test_2016_flickr_set: {
    'en': '${msk}/test_2016_flickr.lc.norm.tok.entity.en',
    'cs': '${tok}/test_2016_flickr.lc.norm.tok.cs',
    'lx': '${lxm}/mask/test_2016_flickr.tok22.obj36.pt'}

[vocabulary]
en: ${global:model}/vocab/train.lc.norm.tok.entity.vocab.en
cs: ${global:model}/vocab/train.lc.norm.tok.vocab.cs
