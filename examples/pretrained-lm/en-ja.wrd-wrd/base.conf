[train]
model_type:      MultimodalTransformer
max_epochs:      500
batch_size:      2000
eval_batch_size: 1500
update_freq:     2

optimizer:       transformer
# set lr==0 to use model_dim^-0.5
lr:              0

eval_metrics:    bleu,loss
eval_beam:       5
eval_freq:       0
eval_max_len:    100
eval_filters:
patience:        10

[model]
# dummy opts; misc.py requires *_dim when using *_type
dec_dim:   512
enc_dim:   512

direction: en,lx:Lxmert->ja
feat_names: lx.l,lx.r
feat_dim:  768
# set feat_ratio==0 to consider only text modality
feat_ratio: 0

bucket_by: en
sampler_type: token

[data]
tok:   ../data/tok
lxm:   ../data/lxmert
brt:   ../data/bert
obj:   ../data/faster_rcnn
msk:   ../data/mask
train_set: {
    'en': '${tok}/train.lc.norm.tok.en',
    'ja': '${tok}/train.mecab.ja',
    'lx': '${lxm}/train_tok22_obj36.pt'}

val_set: {
    'en': '${tok}/val.lc.norm.tok.en',
    'ja': '${tok}/val.mecab.ja',
    'lx': '${lxm}/val_tok22_obj36.pt'}

test_2016_flickr_set: {
    'en': '${tok}/test_2016_flickr.lc.norm.tok.en',
    'ja': '${tok}/test_2016_flickr.mecab.ja',
    'lx': '${lxm}/test_2016_flickr_tok22_obj36.pt'}

[vocabulary]
en: ./vocab/train.lc.norm.tok.vocab.en
ja: ./vocab/train.mecab.vocab.ja
